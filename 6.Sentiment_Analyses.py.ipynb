{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "'''\n",
    "\n",
    "Sentiment analyses of pos.txt and neg.txt file with\n",
    "a collection of positive comments and negative comments\n",
    "This file pre-processes the data. Here is the brief algorithm\n",
    "for pre-processing:\n",
    "\n",
    "1. Get the statements sorted as positive and negative\n",
    "2. Store list of positive and negative statements in separate files\n",
    "and call them pos.txt and neg.txt\n",
    "3. Next create a lexicon of the words in pos and neg statements.\n",
    "To arrive at the lexicon, first tokenize the words in each file.\n",
    "Later convert the words into lower case. Next eliminate those\n",
    "words that occur too frequently and too rarely. The logic is\n",
    "that in either case the impact of these words on the sentiment is less\n",
    "4. Now convert each sentence of pos file into a vector of 1s and 0s\n",
    "using the following logic: First create an empty sentence array of 0s, of\n",
    "length same as the lexicon. Now read each sentence of the positive\n",
    "file. For each word in the sentence, find if there is a\n",
    "matching word in the lexicon. If yes, replace the 0 in the empty sentence array\n",
    "with a 1 at the same index value as the matching word in lexicon.\n",
    "At the end of processing of each sentence, the sentence array\n",
    "will be a collection of 0s and 1s. Each sentence array is of the same\n",
    "length, same as the lexicon and will be filled with 1s and 0s. This is\n",
    "called sentence vector.\n",
    "5. Append the classification identifier to\n",
    "each sentence vector. The classification identifier is an array\n",
    "[1,0] for a positive statement and [0,1] for a negative\n",
    "statement. For example, a sentence vector for a positive sentence\n",
    "will look like [....1,0,0,0,0,0,1....][1,0].\n",
    "6. Repeat the same process for the neg file. The example of\n",
    "a negative sentence vector will look like [....0,0,0,0,1,1,1....][0,1]\n",
    "7. Merge these two list of sentence vectors into a single file and shuffle them.\n",
    "8. Later divide the data set into training set and test set (10%).\n",
    "\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "def create_lexicon(pos,neg):\n",
    "    lexicon= []\n",
    "    with open(pos,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:]:\n",
    "            all_words = word_tokenize(l.lower())\n",
    "            lexicon+= list(all_words)\n",
    "\n",
    "    with open(neg,'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:]:\n",
    "            all_words = word_tokenize(l.lower())\n",
    "            lexicon+= list(all_words)\n",
    "\n",
    "    lexicon = [lemm.lemmatize(i) for i in lexicon]\n",
    "    w_counts = Counter(lexicon)\n",
    "    l2 =[]\n",
    "    for w in w_counts:\n",
    "        if 1000 > w_counts[w] > 5:\n",
    "            l2.append(w)\n",
    "    print('\\n\\n\\n' + 'The length of lexicon is ')\n",
    "    print(len(l2))\n",
    "    print('\\n\\n\\n')\n",
    "    return l2\n",
    "\n",
    "\n",
    "def sample_handling(sample,lexicon,classification):\n",
    "    featureset = []\n",
    "\n",
    "    with open (sample, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents[:]:\n",
    "            current_words = word_tokenize(l.lower())\n",
    "            current_words = [lemm.lemmatize(i) for i in current_words]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            for word in current_words:\n",
    "                if word in lexicon:\n",
    "                    index_value = lexicon.index(word)\n",
    "                    features[index_value] += 1\n",
    "            features = list(features)\n",
    "            featureset.append([features, classification])\n",
    "    return featureset\n",
    "\n",
    "def create_feature_sets_and_labels(pos, neg, test_size=0.1):\n",
    "    lexicon = create_lexicon(pos,neg)\n",
    "    features = []\n",
    "\n",
    "    features += sample_handling('pos.txt', lexicon,[1,0])\n",
    "    features += sample_handling('neg.txt',lexicon,[0,1])\n",
    "\n",
    "    random.shuffle(features)\n",
    "\n",
    "    features = np.array(features)\n",
    "\n",
    "    testing_size = int(test_size*len(features))\n",
    "\n",
    "    train_x = list(features[:,0][:-testing_size])\n",
    "    train_y = list(features[:,1][:-testing_size])\n",
    "\n",
    "    test_x = list(features[:,0][-testing_size:])\n",
    "    test_y = list(features[:,1][-testing_size:])\n",
    "\n",
    "    return train_x,train_y,test_x,test_y\n",
    "\n",
    "train_x,train_y,test_x,test_y = create_feature_sets_and_labels('pos.txt','neg.txt')\n",
    "with open ('sentiment_set.pickle', 'wb') as f:\n",
    "    pickle.dump([train_x,train_y,test_x,test_y],f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
